{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWIU/xP5nRPqpjALyAKm1j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mimrancomsats/ProgrammingforAI_FALL24/blob/main/Lab_15_MLFlow/Lab_15MLflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MLFlow**"
      ],
      "metadata": {
        "id": "B1T4jELFEpTA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in handling the complexities of the machine learning process. MLflow focuses on the full lifecycle for machine learning projects, ensuring that each phase is manageable, traceable, and reproducible.**\n",
        "\n",
        "**In this notebook, we are going to use MLFlow for two purposes: Experiment Tracking and Model Inference**\n",
        "\n",
        "*   **Experiment Tracking**\n",
        "*   **Model Inference**\n",
        "\n",
        "**The installation process of MLFlow is described in the following link:**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "https://mlflow.org/docs/latest/getting-started/intro-quickstart/index.html"
      ],
      "metadata": {
        "id": "Ip9w5YHZEu7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MLFlow Library Installation**"
      ],
      "metadata": {
        "id": "FmZxJ3R3uLUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet mlflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XEMbJcXdYLAR",
        "outputId": "a6031220-1d5f-4b7e-9186-06cca913f70b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.4/27.4 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m623.0/623.0 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sklearn Pipeline Implementation (KNN)**"
      ],
      "metadata": {
        "id": "0v6QLYCxh2mZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('titanic.csv')\n",
        "\n",
        "# Custom function to impute missing values in 'Embarked' column\n",
        "def impute_embarked(X):\n",
        "    X['Embarked'] = X['Embarked'].fillna(X['Embarked'].mode()[0])  # Fill missing values\n",
        "    return X\n",
        "\n",
        "# Custom function to create the 'FamilySize' feature\n",
        "def create_family_size(X):\n",
        "    X['FamilySize'] = X['SibSp'] + X['Parch'] + 1  # Add 1 for the individual themselves\n",
        "    return X\n",
        "\n",
        "# Custom function to drop columns that are not needed for model training\n",
        "def drop_columns(X):\n",
        "    return X.drop(['SibSp', 'Parch'], axis=1)\n",
        "\n",
        "# Function to create 'FamilySize' and drop 'SibSp' and 'Parch' columns\n",
        "def family_size(X):\n",
        "    X = create_family_size(X)\n",
        "    X = drop_columns(X)\n",
        "    return X\n",
        "\n",
        "# Pipeline to preprocess 'Age' column\n",
        "age_pipeline = Pipeline(steps=[\n",
        "    ('age_imputer', SimpleImputer(strategy='mean')),  # Impute missing 'Age' values\n",
        "    ('age_scaler', MinMaxScaler())  # Scale 'Age' feature\n",
        "])\n",
        "\n",
        "# Pipeline to preprocess 'Fare' column\n",
        "fare_pipeline = Pipeline(steps=[\n",
        "    #('fare_imputer', SimpleImputer(strategy='mean')),  # Optionally impute missing 'Fare'\n",
        "    ('fare_scaler', MinMaxScaler())  # Scale 'Fare' feature\n",
        "])\n",
        "\n",
        "# Pipeline to create and scale the 'FamilySize' feature\n",
        "family_size_pipeline = Pipeline(steps=[\n",
        "    ('family_size_creator', FunctionTransformer(family_size)),\n",
        "    ('family_size_scaler', MinMaxScaler()),  # Scale 'FamilySize'\n",
        "])\n",
        "\n",
        "# Pipeline to preprocess 'Embarked' column\n",
        "embarked_pipeline = Pipeline(steps=[\n",
        "    ('embarked_imputer', FunctionTransformer(impute_embarked)),  # Impute missing 'Embarked' values\n",
        "    ('embarked_onehot', OneHotEncoder())  # One-hot encode 'Embarked'\n",
        "])\n",
        "\n",
        "# Create a ColumnTransformer to preprocess all relevant features\n",
        "knn_preprocessor = ColumnTransformer(transformers=[\n",
        "    ('drop', 'drop', ['PassengerId', 'Name', 'Ticket', 'Cabin']),  # Drop irrelevant columns\n",
        "    ('age_encoder', age_pipeline, ['Age']),  # Preprocess 'Age'\n",
        "    ('fare_encoder', fare_pipeline, ['Fare']),  # Preprocess 'Fare'\n",
        "    ('family_size', family_size_pipeline, ['SibSp', 'Parch']),  # Preprocess 'FamilySize'\n",
        "    ('embarked_encoder', embarked_pipeline, ['Embarked']),  # Preprocess 'Embarked'\n",
        "    ('sex_encoder', OneHotEncoder(), ['Sex']),  # One-hot encode 'Sex'\n",
        "    ('pclass_scaler', MinMaxScaler(), ['Pclass']),  # Scale 'Pclass'\n",
        "], remainder='passthrough')\n",
        "\n",
        "# Create a complete pipeline with preprocessing and the KNN classifier\n",
        "knn_pipeline = Pipeline(steps=[\n",
        "    ('knn_preprocessor', knn_preprocessor),  # Data preprocessing steps\n",
        "    ('knn_classifier', KNeighborsClassifier(n_neighbors=5))  # KNN Classifier\n",
        "])\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop('Survived', axis=1)\n",
        "y = data['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "knn_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = knn_pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "knn_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nKNN Model Accuracy: {knn_accuracy:.2f}\")\n",
        "\n",
        "# Confusion matrix for evaluating the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Classification report (detailed evaluation)\n",
        "#knn_report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
        "#print(\"\\nKNN Classification Report:\")\n",
        "#print(knn_report_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2FbXPYah0pj",
        "outputId": "b523b7e1-9421-4a7c-e9bb-772a399667fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "KNN Model Accuracy: 0.80\n",
            "Confusion Matrix:\n",
            "[[90 15]\n",
            " [21 53]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Experiment Tracking**"
      ],
      "metadata": {
        "id": "JDTcZM1lupfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "# Set the tracking URI and experiment name\n",
        "mlflow.set_tracking_uri(uri=\"http://3.91.21.217:5000\")\n",
        "mlflow.set_experiment(\"KNN Experiment\")\n",
        "\n",
        "# Start a new MLflow run\n",
        "with mlflow.start_run():\n",
        "\n",
        "    # Log the prameters related to KNN model\n",
        "    mlflow.log_param(\"model\",\"KNN\")\n",
        "    mlflow.log_param(\"n_neighbors\", 5)\n",
        "    mlflow.log_param(\"metric\", 'minkowski')\n",
        "\n",
        "    # Log the accuracy metric\n",
        "    mlflow.log_metric(\"accuracy\", knn_accuracy)\n",
        "\n",
        "    # Log the KNN model (use the knn_pipeline variable)\n",
        "    mlflow.sklearn.log_model(knn_pipeline, \"KNN Algorithm\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XBnh9LaXb3K",
        "outputId": "325abdd2-6fc0-462f-83c4-04cce6630c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024/12/18 12:35:16 INFO mlflow.tracking.fluent: Experiment with name 'KNN Experiment' does not exist. Creating a new experiment.\n",
            "\u001b[31m2024/12/18 12:35:36 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏃 View run useful-eel-882 at: http://3.91.21.217:5000/#/experiments/592108578757940918/runs/ce6c402698ad4d34bffa684e4d086fba\n",
            "🧪 View experiment at: http://3.91.21.217:5000/#/experiments/592108578757940918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sklearn Pipeline Implementation (Decision Tree)**"
      ],
      "metadata": {
        "id": "HscZRUP0G7xZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('titanic.csv')\n",
        "\n",
        "# Custom function to impute missing values in the 'Embarked' column\n",
        "def impute_embarked(X):\n",
        "    X['Embarked'] = X['Embarked'].fillna(X['Embarked'].mode()[0])  # Fill missing values\n",
        "    return X\n",
        "\n",
        "# Custom function to create the 'FamilySize' feature\n",
        "def create_family_size(X):\n",
        "    X['FamilySize'] = X['SibSp'] + X['Parch'] + 1  # Adding 1 for the individual themselves\n",
        "    return X\n",
        "\n",
        "# Custom function to drop specified columns\n",
        "def drop_columns(X):\n",
        "    return X.drop(['SibSp', 'Parch'], axis=1)\n",
        "\n",
        "# Function to create 'FamilySize' and drop 'SibSp' and 'Parch' columns\n",
        "def family_size(X):\n",
        "    X = create_family_size(X)\n",
        "    X = drop_columns(X)\n",
        "    return X\n",
        "\n",
        "# Create pipelines for 'Age'\n",
        "age_pipeline = Pipeline(steps=[\n",
        "    ('age_imputer', SimpleImputer(strategy='mean')),  # Impute missing 'Age' values\n",
        "    ('age_scaler', MinMaxScaler())  # Scale 'Age' feature\n",
        "])\n",
        "\n",
        "# Create pipelines for 'Fare'\n",
        "fare_pipeline = Pipeline(steps=[\n",
        "    ('fare_scaler', MinMaxScaler())  # Scale 'Fare' feature\n",
        "])\n",
        "\n",
        "# Create pipelines for 'FamilySize'\n",
        "family_size_pipeline = Pipeline(steps=[\n",
        "    ('family_size_creator', FunctionTransformer(family_size)),\n",
        "    ('family_size_scaler', MinMaxScaler())  # Scale 'FamilySize' feature\n",
        "])\n",
        "\n",
        "# Create pipelines for 'Embarked'\n",
        "embarked_pipeline = Pipeline(steps=[\n",
        "    ('embarked_imputer', FunctionTransformer(impute_embarked)),  # Impute missing 'Embarked' values\n",
        "    ('embarked_onehot', OneHotEncoder())  # One-hot encode 'Embarked'\n",
        "])\n",
        "\n",
        "# Create a ColumnTransformer to preprocess the data\n",
        "dt_preprocessor = ColumnTransformer(transformers=[\n",
        "    ('drop', 'drop', ['PassengerId', 'Name', 'Ticket', 'Cabin']),  # Drop irrelevant columns\n",
        "    ('age_encoder', age_pipeline, ['Age']),  # Preprocess 'Age'\n",
        "    ('fare_encoder', fare_pipeline, ['Fare']),  # Preprocess 'Fare'\n",
        "    ('family_size', family_size_pipeline, ['SibSp', 'Parch']),  # Preprocess 'FamilySize'\n",
        "    ('embarked_encoder', embarked_pipeline, ['Embarked']),  # Preprocess 'Embarked'\n",
        "    ('sex_encoder', OneHotEncoder(), ['Sex']),  # One-hot encode 'Sex'\n",
        "    ('pclass_scaler', MinMaxScaler(), ['Pclass']),  # Scale 'Pclass'\n",
        "], remainder='passthrough')\n",
        "\n",
        "# Create a complete pipeline that includes preprocessing and the Decision Tree classifier\n",
        "dt_pipeline = Pipeline(steps=[\n",
        "    ('dt_preprocessor', dt_preprocessor),  # Data preprocessing steps\n",
        "    ('dt_classifier', DecisionTreeClassifier(random_state=42))  # Decision Tree Classifier\n",
        "])\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop('Survived', axis=1)\n",
        "y = data['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "dt_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dt_pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "dt_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nDecision Tree Model Accuracy: {dt_accuracy:.2f}\")\n",
        "\n",
        "# Confusion matrix for evaluating the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Classification report (detailed evaluation)\n",
        "#dt_report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
        "#print(\"\\nDecision Tree Classification Report:\")\n",
        "#print(dt_report_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcB_MACF8LDc",
        "outputId": "a004970e-1475-4d5e-85a4-d0b7a8418994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Decision Tree Model Accuracy: 0.77\n",
            "Confusion Matrix:\n",
            "[[82 23]\n",
            " [19 55]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Experiment Tracking**"
      ],
      "metadata": {
        "id": "ypLDHQ3Kuwfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "# Set the tracking URI and experiment name\n",
        "mlflow.set_tracking_uri(uri=\"http://3.91.21.217:5000\")\n",
        "mlflow.set_experiment(\"Decision Tree Experiment\")\n",
        "\n",
        "# Start a new MLflow run\n",
        "with mlflow.start_run():\n",
        "\n",
        "    # Log the parameters related to Decision Tree model\n",
        "    mlflow.log_param(\"model\",\"Decision Tree\")\n",
        "    mlflow.log_param(\"criterion\", \"gini\")\n",
        "    mlflow.log_param(\"random_state\", 42)\n",
        "\n",
        "    # Log the accuracy metric\n",
        "    mlflow.log_metric(\"accuracy\", dt_accuracy)\n",
        "\n",
        "    # Log the Decision Tree model (use the dt_pipeline variable)\n",
        "    mlflow.sklearn.log_model(dt_pipeline, \"Decision Tree Algorithm\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gvzvmw6X8PP2",
        "outputId": "3879cede-e5ed-4a7e-d990-6885644900e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024/12/18 12:35:38 INFO mlflow.tracking.fluent: Experiment with name 'Decision Tree Experiment' does not exist. Creating a new experiment.\n",
            "\u001b[31m2024/12/18 12:35:42 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏃 View run able-cod-526 at: http://3.91.21.217:5000/#/experiments/191437345984426865/runs/0ca2dc6c60344c70a8513a409e7d1b3f\n",
            "🧪 View experiment at: http://3.91.21.217:5000/#/experiments/191437345984426865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sklearn Pipeline Implementation (Random Forest)**"
      ],
      "metadata": {
        "id": "TA2ZdeprHDEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier  # Import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('titanic.csv')\n",
        "\n",
        "# Custom function to impute missing values in the 'Embarked' column\n",
        "def impute_embarked(X):\n",
        "    X['Embarked'] = X['Embarked'].fillna(X['Embarked'].mode()[0])  # Fill missing values\n",
        "    return X\n",
        "\n",
        "# Custom function to create the 'FamilySize' feature\n",
        "def create_family_size(X):\n",
        "    X['FamilySize'] = X['SibSp'] + X['Parch'] + 1  # Adding 1 for the individual themselves\n",
        "    return X\n",
        "\n",
        "# Custom function to drop specified columns\n",
        "def drop_columns(X):\n",
        "    return X.drop(['SibSp', 'Parch'], axis=1)\n",
        "\n",
        "# Function to create 'FamilySize' and drop 'SibSp' and 'Parch' columns\n",
        "def family_size(X):\n",
        "    X = create_family_size(X)\n",
        "    X = drop_columns(X)\n",
        "    return X\n",
        "\n",
        "# Create pipelines for 'Age'\n",
        "age_pipeline = Pipeline(steps=[\n",
        "    ('age_imputer', SimpleImputer(strategy='mean')),  # Impute missing 'Age' values\n",
        "    ('age_scaler', MinMaxScaler())  # Scale 'Age' feature\n",
        "])\n",
        "\n",
        "# Create pipelines for 'Fare'\n",
        "fare_pipeline = Pipeline(steps=[\n",
        "    ('fare_scaler', MinMaxScaler())  # Scale 'Fare' feature\n",
        "])\n",
        "\n",
        "# Create pipelines for 'FamilySize'\n",
        "family_size_pipeline = Pipeline(steps=[\n",
        "    ('family_size_creator', FunctionTransformer(family_size)),\n",
        "    ('family_size_scaler', MinMaxScaler())  # Scale 'FamilySize' feature\n",
        "])\n",
        "\n",
        "# Create pipelines for 'Embarked'\n",
        "embarked_pipeline = Pipeline(steps=[\n",
        "    ('embarked_imputer', FunctionTransformer(impute_embarked)),  # Impute missing 'Embarked' values\n",
        "    ('embarked_onehot', OneHotEncoder())  # One-hot encode 'Embarked'\n",
        "])\n",
        "\n",
        "# Create a ColumnTransformer to preprocess the data\n",
        "rf_preprocessor = ColumnTransformer(transformers=[\n",
        "    ('drop', 'drop', ['PassengerId', 'Name', 'Ticket', 'Cabin']),  # Drop irrelevant columns\n",
        "    ('age_encoder', age_pipeline, ['Age']),  # Preprocess 'Age'\n",
        "    ('fare_encoder', fare_pipeline, ['Fare']),  # Preprocess 'Fare'\n",
        "    ('family_size', family_size_pipeline, ['SibSp', 'Parch']),  # Preprocess 'FamilySize'\n",
        "    ('embarked_encoder', embarked_pipeline, ['Embarked']),  # Preprocess 'Embarked'\n",
        "    ('sex_encoder', OneHotEncoder(), ['Sex']),  # One-hot encode 'Sex'\n",
        "    ('pclass_scaler', MinMaxScaler(), ['Pclass']),  # Scale 'Pclass'\n",
        "], remainder='passthrough')\n",
        "\n",
        "# Create a complete pipeline that includes preprocessing and the Random Forest classifier\n",
        "rf_pipeline = Pipeline(steps=[\n",
        "    ('rf_preprocessor', rf_preprocessor),  # Data preprocessing steps\n",
        "    ('rf_classifier', RandomForestClassifier(random_state=42))  # Random Forest Classifier\n",
        "])\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop('Survived', axis=1)\n",
        "y = data['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "rf_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "rf_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nRandom Forest Model Accuracy: {rf_accuracy:.2f}\")\n",
        "\n",
        "# Confusion matrix for evaluating the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Classification report (detailed evaluation)\n",
        "#rf_report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
        "#print(\"\\nRandom Forest Classification Report:\")\n",
        "#print(rf_report_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAy0Iw6Y-SoE",
        "outputId": "6de606b1-7bc5-451d-cb6c-76ead3ac2fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Random Forest Model Accuracy: 0.82\n",
            "Confusion Matrix:\n",
            "[[91 14]\n",
            " [19 55]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Experiment Tracking**"
      ],
      "metadata": {
        "id": "X2B-ns2ou0Me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "# Set the tracking URI and experiment name for Random Forest\n",
        "mlflow.set_tracking_uri(uri=\"http://3.91.21.217:5000\")\n",
        "mlflow.set_experiment(\"Random Forest Experiment\")\n",
        "\n",
        "# Start a new MLflow run\n",
        "with mlflow.start_run():\n",
        "\n",
        "    # Log the hyperparameters\n",
        "    mlflow.log_param(\"model\",\"Random Forest\")\n",
        "    mlflow.log_param(\"n_estimators\", 100)\n",
        "    mlflow.log_param(\"random_state\", 42)\n",
        "\n",
        "    # Log the accuracy metric\n",
        "    mlflow.log_metric(\"accuracy\", rf_accuracy)\n",
        "\n",
        "    # Log the Random Forest model (use the rf_pipeline variable)\n",
        "    mlflow.sklearn.log_model(rf_pipeline, \"Random Forest Algorithm\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeeduLjA-fdd",
        "outputId": "045d2181-1ea1-48f5-8138-dac1ceff32c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024/12/18 12:35:45 INFO mlflow.tracking.fluent: Experiment with name 'Random Forest Experiment' does not exist. Creating a new experiment.\n",
            "\u001b[31m2024/12/18 12:35:49 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏃 View run efficient-gnat-96 at: http://3.91.21.217:5000/#/experiments/532351182075092251/runs/ab56cd620c6449e3928b39184d0ccc05\n",
            "🧪 View experiment at: http://3.91.21.217:5000/#/experiments/532351182075092251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sklearn Pipeline Implementation (ANN)**"
      ],
      "metadata": {
        "id": "OklxFjpPMYWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('titanic.csv')\n",
        "\n",
        "# Define custom functions for preprocessing\n",
        "def impute_embarked(X):\n",
        "    X['Embarked'] = X['Embarked'].fillna(X['Embarked'].mode()[0])  # Fill missing values\n",
        "    return X\n",
        "\n",
        "def create_family_size(X):\n",
        "    X['FamilySize'] = X['SibSp'] + X['Parch'] + 1  # Adding 1 for the individual themselves\n",
        "    return X\n",
        "\n",
        "def drop_columns(X):\n",
        "    return X.drop(['SibSp', 'Parch'], axis=1)\n",
        "\n",
        "def family_size(X):\n",
        "    X = create_family_size(X)\n",
        "    X = drop_columns(X)\n",
        "    return X\n",
        "\n",
        "# Create pipelines for different features\n",
        "age_pipeline = Pipeline(steps=[\n",
        "    ('age_imputer', SimpleImputer(strategy='mean')),  # Impute Age\n",
        "    ('age_scaler', MinMaxScaler())  # Scale Age\n",
        "])\n",
        "\n",
        "fare_pipeline = Pipeline(steps=[\n",
        "    ('fare_scaler', MinMaxScaler())  # Scale Fare\n",
        "])\n",
        "\n",
        "family_size_pipeline = Pipeline(steps=[\n",
        "    ('family_size_creator', FunctionTransformer(family_size)),\n",
        "    ('family_size_scaler', MinMaxScaler()),  # Scale Family_Size\n",
        "])\n",
        "\n",
        "embarked_pipeline = Pipeline(steps=[\n",
        "    ('embarked_imputer', FunctionTransformer(impute_embarked)),  # Impute Embarked\n",
        "    ('embarked_onehot', OneHotEncoder())  # One-hot encode Embarked\n",
        "])\n",
        "\n",
        "# Column transformer for preprocessing\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('drop', 'drop', ['PassengerId', 'Name', 'Ticket', 'Cabin']),\n",
        "    ('age_encoder', age_pipeline, ['Age']),\n",
        "    ('fare_encoder', fare_pipeline, ['Fare']),\n",
        "    ('family_size', family_size_pipeline, ['SibSp', 'Parch']),  # Process FamilySize\n",
        "    ('embarked_encoder', embarked_pipeline, ['Embarked']),\n",
        "    ('sex_encoder', OneHotEncoder(), ['Sex']),\n",
        "    ('scaler', MinMaxScaler(), ['Pclass']),  # Scale Pclass\n",
        "], remainder='passthrough')\n",
        "\n",
        "# Build the ANN model\n",
        "def ann_model(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=input_shape), # Input layer\n",
        "        tf.keras.layers.Dense(64, activation='relu'),  # Hidden layer\n",
        "        tf.keras.layers.Dense(32, activation='relu'),  # Hidden layer\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer (binary classification)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Prepare the data\n",
        "X = data.drop('Survived', axis=1)\n",
        "y = data['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocess the data (without including the classifier in the pipeline)\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "print(f\"X_train shape: {X_train_processed.shape}\")\n",
        "print(f\"X_test shape: {X_test_processed.shape}\")\n",
        "\n",
        "# Print the first 5 rows of X_train_processed\n",
        "#print(X_train_processed[:5])\n",
        "\n",
        "\n",
        "# Build the ANN model\n",
        "model = ann_model(input_shape=(X_train_processed.shape[1],))\n",
        "print(f\"Model input shape: {model.input_shape}\")\n",
        "\n",
        "#Model Summary\n",
        "print(model.summary())\n",
        "\n",
        "# Compile the ANN model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the ANN model\n",
        "model.fit(X_train_processed, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_processed)\n",
        "y_pred = (y_pred > 0.5).astype(\"int32\")  # Convert predictions to 0 or 1\n",
        "\n",
        "# Evaluate the model\n",
        "ann_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy: {ann_accuracy:.2f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "id": "LazmsXz5MrjV",
        "outputId": "4c1694f7-0a06-43dd-81ee-c58c40fb9a85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (712, 9)\n",
            "X_test shape: (179, 9)\n",
            "Model input shape: (None, 9)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m640\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,753\u001b[0m (10.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,753</span> (10.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,753\u001b[0m (10.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,753</span> (10.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.7216 - loss: 0.6370 - val_accuracy: 0.8252 - val_loss: 0.5446\n",
            "Epoch 2/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7669 - loss: 0.5724 - val_accuracy: 0.8601 - val_loss: 0.4869\n",
            "Epoch 3/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7856 - loss: 0.5291 - val_accuracy: 0.8252 - val_loss: 0.4489\n",
            "Epoch 4/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7838 - loss: 0.5037 - val_accuracy: 0.8322 - val_loss: 0.4322\n",
            "Epoch 5/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8148 - loss: 0.4575 - val_accuracy: 0.8322 - val_loss: 0.4172\n",
            "Epoch 6/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7926 - loss: 0.4681 - val_accuracy: 0.8322 - val_loss: 0.4210\n",
            "Epoch 7/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7977 - loss: 0.4651 - val_accuracy: 0.8322 - val_loss: 0.4099\n",
            "Epoch 8/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7981 - loss: 0.4678 - val_accuracy: 0.8322 - val_loss: 0.4065\n",
            "Epoch 9/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8187 - loss: 0.4550 - val_accuracy: 0.8322 - val_loss: 0.4031\n",
            "Epoch 10/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8053 - loss: 0.4767 - val_accuracy: 0.8462 - val_loss: 0.3997\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\n",
            "Model Accuracy: 0.79\n",
            "Confusion Matrix:\n",
            "[[92 13]\n",
            " [24 50]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Experiment Tracking**"
      ],
      "metadata": {
        "id": "M2-hJ0Ulu4Be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "# Set the tracking URI and experiment name for Random Forest\n",
        "mlflow.set_tracking_uri(uri=\"http://3.91.21.217:5000\")\n",
        "mlflow.set_experiment(\"ANN Experiment\")\n",
        "\n",
        "# Start a new MLflow run\n",
        "with mlflow.start_run():\n",
        "\n",
        "    # Log the hyperparameters\n",
        "    mlflow.log_param(\"model\",\"ANN\")\n",
        "    mlflow.log_param(\"hidden_layers\", 2)\n",
        "    mlflow.log_param(\"optimizer\", \"adam\")\n",
        "    mlflow.log_param(\"batch_size\", 32)\n",
        "    mlflow.log_param(\"learning_rate\", \"constant\")\n",
        "    mlflow.log_param(\"learning_rate_init\", 0.001)\n",
        "    mlflow.log_param(\"epochs\", 10)\n",
        "    mlflow.log_param(\"verbose\", True)\n",
        "    mlflow.log_param(\"validation_fraction\", 0.2)\n",
        "\n",
        "\n",
        "    # Log the accuracy metric\n",
        "    mlflow.log_metric(\"accuracy\", ann_accuracy)\n",
        "\n",
        "    # Log the ANN model (use the rf_pipeline variable)\n",
        "    mlflow.sklearn.log_model(model, \"ANN Algorithm\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Osq0SoQ5Y-FL",
        "outputId": "24453db7-03ed-4f8b-a0cf-8520d3e0a5c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024/12/18 12:36:00 INFO mlflow.tracking.fluent: Experiment with name 'ANN Experiment' does not exist. Creating a new experiment.\n",
            "2024/12/18 12:36:10 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2024-12-04; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'google-genai'}\n",
            "\u001b[31m2024/12/18 12:36:10 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏃 View run sophisticated-pug-39 at: http://3.91.21.217:5000/#/experiments/294652062718827529/runs/197d438e29e34d2a9106d88365c63c12\n",
            "🧪 View experiment at: http://3.91.21.217:5000/#/experiments/294652062718827529\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Inference**"
      ],
      "metadata": {
        "id": "2GekIgvNt26c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model back for predictions as a generic Python Function model\n",
        "# Cell for ubuntu-MLFlow EC2 Machine.\n",
        "#model_uri = f\"models:/Random Forest@champion\"\n",
        "loaded_model = mlflow.pyfunc.load_model(\"models:/KNN/1\")\n",
        "\n",
        "predictions = loaded_model.predict(X_test)\n",
        "#print(type(predictions))\n",
        "\n",
        "result = pd.DataFrame(X_teast)\n",
        "result[\"actual_class\"] = y_test\n",
        "result[\"predicted_class\"] = predictions\n",
        "\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "result[:20]\n",
        "#result.head()"
      ],
      "metadata": {
        "id": "fY3zJM7MWgCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab Task\n",
        "\n",
        "Perform the steps mentioned above on the following dataset\n",
        "\n",
        "https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease"
      ],
      "metadata": {
        "id": "Vu7nZz_DrFFY"
      }
    }
  ]
}